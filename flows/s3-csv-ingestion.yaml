id: s3-csv-ingestion
namespace: com.flowio
description: "Get CSV from S3 and insert into TimescaleDB, ClickHouse, Elasticsearch, and Qdrant"

inputs:
  - id: s3_bucket
    type: STRING
    defaults: "static-staging.flowio.app"
  - id: s3_key
    type: STRING
    defaults: "resources/kestra-test/sample-metrics.csv"
  - id: table_name
    type: STRING
    defaults: "test_metrics_data"
  - id: delimiter
    type: STRING
    defaults: ","

tasks:
  - id: download_file
    type: io.kestra.plugin.aws.s3.Download
    description: "Download CSV from S3"
    bucket: "{{ inputs.s3_bucket }}"
    key: "{{ inputs.s3_key }}"
    accessKeyId: "{{ secret('AWS_ACCESS_KEY_ID') }}"
    secretKeyId: "{{ secret('AWS_SECRET_ACCESS_KEY') }}"
    region: "{{ secret('AWS_REGION') }}"

  - id: create_tables
    type: io.kestra.plugin.core.flow.Parallel
    description: "Create tables in databases"
    tasks:
      - id: create_timescaledb_table
        type: io.kestra.plugin.jdbc.postgresql.Query
        description: "Create TimescaleDB table if not exists"
        url: "jdbc:postgresql://postgres:5432/kestra_data"
        username: "kestra_user"
        password: "kestra_password"
        sql: |
          CREATE TABLE IF NOT EXISTS {{ inputs.table_name }} (
            timestamp TIMESTAMPTZ,
            device_id TEXT,
            metric_name TEXT,
            metric_value DOUBLE PRECISION,
            location TEXT,
            zone TEXT
          );
          SELECT create_hypertable('{{ inputs.table_name }}', 'timestamp', if_not_exists => TRUE);

      - id: create_clickhouse_table
        type: io.kestra.plugin.scripts.python.Script
        description: "Create ClickHouse table using Python HTTP API"
        script: |
          import urllib.request
          import urllib.parse
          import base64

          clickhouse_host = "host.docker.internal" 
          clickhouse_port = "8123"
          username = "{{ secret('CLICKHOUSE_USER') }}"
          password = "{{ secret('CLICKHOUSE_PASSWORD') }}"
          database = "kestra_data"
          table = "{{ inputs.table_name }}"

          url = f"http://{clickhouse_host}:{clickhouse_port}/"
          
          create_db_query = f"CREATE DATABASE IF NOT EXISTS {database}"
          params = urllib.parse.urlencode({'query': create_db_query})
          full_url = f"{url}?{params}"
          
          req = urllib.request.Request(full_url, method='POST')
          if password:
              credentials = f"{username}:{password}"
              encoded_credentials = base64.b64encode(credentials.encode()).decode()
              req.add_header('Authorization', f'Basic {encoded_credentials}')
          
          try:
              response = urllib.request.urlopen(req)
              print(f"Database {database} created/verified successfully")
          except Exception as e:
              print(f"Error creating database: {e}")
          
          create_table_query = f"""
          CREATE TABLE IF NOT EXISTS {database}.{table} (
              timestamp DateTime64(3),
              device_id String,
              metric_name String,
              metric_value Float64,
              location String,
              zone String
          ) ENGINE = MergeTree()
          ORDER BY (timestamp, device_id, metric_name)
          PARTITION BY toYYYYMM(timestamp)
          TTL timestamp + INTERVAL 1 YEAR
          """
          
          params = urllib.parse.urlencode({'query': create_table_query})
          full_url = f"{url}?{params}"
          
          req = urllib.request.Request(full_url, method='POST')
          if password:
              credentials = f"{username}:{password}"
              encoded_credentials = base64.b64encode(credentials.encode()).decode()
              req.add_header('Authorization', f'Basic {encoded_credentials}')
          
          try:
              response = urllib.request.urlopen(req)
              print(f"Table {table} created/verified successfully")
              print("clickhouse_table_status=success")
          except Exception as e:
              print(f"Error creating table: {e}")
              print("clickhouse_table_status=error")
              print(f"clickhouse_table_error={str(e)}")

      - id: create_qdrant_collection
        type: io.kestra.plugin.scripts.python.Script
        description: "Create Qdrant collection if not exists"
        script: |
          import urllib.request
          import json

          qdrant_host = "host.docker.internal"
          qdrant_port = "6333"
          collection_name = "{{ inputs.table_name }}"

          def create_collection_if_not_exists():
              """Create Qdrant collection if it doesn't exist"""
              try:
                  # Check if collection exists
                  url = f"http://{qdrant_host}:{qdrant_port}/collections/{collection_name}"
                  print(f"Checking collection at: {url}")
                  req = urllib.request.Request(url, method='GET')
                  
                  try:
                      response = urllib.request.urlopen(req)
                      print(f"Collection {collection_name} already exists")
                      return True
                  except urllib.error.HTTPError as e:
                      if e.code == 404:
                          # Create collection - use correct API endpoint
                          create_url = f"http://{qdrant_host}:{qdrant_port}/collections/{collection_name}"
                          print(f"Creating collection at: {create_url}")
                          
                          collection_config = {
                              "vectors": {
                                  "size": 6,  # 6 dimensions for our metrics
                                  "distance": "Cosine"
                              },
                              "on_disk_payload": True
                          }
                          
                          print(f"Collection config: {json.dumps(collection_config, indent=2)}")
                          
                          req = urllib.request.Request(create_url, method='PUT')
                          req.add_header('Content-Type', 'application/json')
                          
                          response = urllib.request.urlopen(req, data=json.dumps(collection_config).encode())
                          print(f"Collection {collection_name} created successfully")
                          return True
                      else:
                          print(f"Error checking collection: {e}")
                          return False
              except Exception as e:
                  print(f"Error creating collection: {e}")
                  print(f"Full error details: {type(e).__name__}: {str(e)}")
                  return False

          if create_collection_if_not_exists():
              print("qdrant_collection_status=success")
          else:
              print("qdrant_collection_status=error")
              print("qdrant_collection_error=Failed to create collection")

  - id: transform_data
    type: io.kestra.plugin.core.flow.Parallel
    description: "Transform data and insert into databases"
    tasks:
      - id: copy_data_to_timescaledb
        type: io.kestra.plugin.jdbc.postgresql.CopyIn
        description: "Insert CSV data into TimescaleDB"
        url: "jdbc:postgresql://postgres:5432/kestra_data"
        username: "kestra_user"
        password: "kestra_password"
        format: CSV
        from: "{{ outputs.download_file.uri }}"
        table: "{{ inputs.table_name }}"
        header: true
        delimiter: "{{ inputs.delimiter }}"
        columns:
          - timestamp
          - device_id
          - metric_name
          - metric_value
          - location
          - zone

      - id: copy_data_to_clickhouse
        type: io.kestra.plugin.scripts.python.Script
        description: "Insert CSV data into ClickHouse using Python HTTP API"
        script: |
          import csv
          import urllib.request
          import urllib.parse
          import base64
          from pathlib import Path
          from datetime import datetime
          
          clickhouse_host = "host.docker.internal"  # Docker host from Python container
          clickhouse_port = "8123"
          username = "{{ secret('CLICKHOUSE_USER') }}"
          password = "{{ secret('CLICKHOUSE_PASSWORD') }}"
          database = "kestra_data"
          table = "{{ inputs.table_name }}"
          batch_size = 10000
          
          def insert_batch(batch_data, db, tbl, host, port, user, pwd):
              """Insert a batch of data into ClickHouse"""
              try:
                  url = f"http://{host}:{port}/"
                  
                  insert_query = f"INSERT INTO {db}.{tbl} (timestamp, device_id, metric_name, metric_value, location, zone) VALUES"
                  
                  values_str = ""
                  for row in batch_data:
                      values_str += f"('{row[0]}', '{row[1]}', '{row[2]}', {row[3]}, '{row[4]}', '{row[5]}'),"
                  
                  values_str = values_str.rstrip(',')
                  full_query = f"{insert_query} {values_str}"
                  
                  params = urllib.parse.urlencode({'query': full_query})
                  full_url = f"{url}?{params}"
                  
                  req = urllib.request.Request(full_url, method='POST')
                  if pwd:
                      credentials = f"{user}:{pwd}"
                      encoded_credentials = base64.b64encode(credentials.encode()).decode()
                      req.add_header('Authorization', f'Basic {encoded_credentials}')
                  
                  print(f"Successfully inserted batch of {len(batch_data)} rows")
                  return True
                  
              except Exception as e:
                  print(f"Error inserting batch: {e}")
                  return False
          
          csv_path = "{{ outputs.download_file.uri }}"
          
          total_rows = 0
          successful_inserts = 0
          
          with open(csv_path, 'r', encoding='utf-8') as file:
              reader = csv.DictReader(file, delimiter='{{ inputs.delimiter }}')
              
              batch = []
              for row in reader:
                  total_rows += 1
                  
                  try:
                      timestamp = datetime.fromisoformat(row['timestamp'].replace('Z', '+00:00'))
                      timestamp_str = timestamp.strftime('%Y-%m-%d %H:%M:%S')
                  except:
                      timestamp_str = row['timestamp']
                  
                  batch.append([
                      timestamp_str,
                      row['device_id'],
                      row['metric_name'],
                      float(row['metric_value']),
                      row['location'],
                      row['zone']
                  ])
                  
                  if len(batch) >= batch_size:
                      if insert_batch(batch, database, table, clickhouse_host, clickhouse_port, username, password):
                          successful_inserts += len(batch)
                      batch = []
                      print(f"Processed {total_rows} rows so far...")
              
              if batch:
                  if insert_batch(batch, database, table, clickhouse_host, clickhouse_port, username, password):
                      successful_inserts += len(batch)
          
          print(f"Total rows processed: {total_rows}")
          print(f"Successfully inserted: {successful_inserts}")
          
          print(f"clickhouse_rows_inserted={successful_inserts}")
          print("clickhouse_status=success" if successful_inserts > 0 else "clickhouse_status=error")
          if successful_inserts == 0:
              print("clickhouse_error=No rows were inserted successfully")

      - id: copy_data_to_elasticsearch
        type: io.kestra.plugin.scripts.python.Script
        description: "Bulk load CSV data into Elasticsearch using Python"
        script: |
          import csv
          import json
          import urllib.request
          import urllib.parse
          import base64
          from datetime import datetime
          
          es_host = "host.docker.internal"  # Docker host from Python container
          es_port = "9200"
          username = "{{ secret('ES_USER') }}"
          password = "{{ secret('ES_PASSWORD') }}"
          index_name = "{{ inputs.table_name }}"
          
          csv_path = "{{ outputs.download_file.uri }}"
          
          def create_index_if_not_exists():
              """Create Elasticsearch index if it doesn't exist"""
              try:
                  url = f"http://{es_host}:{es_port}/{index_name}"
                  req = urllib.request.Request(url, method='HEAD')
                  if password:
                      credentials = f"{username}:{password}"
                      encoded_credentials = base64.b64encode(credentials.encode()).decode()
                      req.add_header('Authorization', f'Basic {encoded_credentials}')
                      
                  response = urllib.request.urlopen(req)
                  print(f"Index {index_name} already exists")
                  return True
              except urllib.error.HTTPError as e:
                  if e.code == 404:
                      try:
                          create_url = f"http://{es_host}:{es_port}/{index_name}"
                          mapping = {
                              "mappings": {
                                  "properties": {
                                      "timestamp": {"type": "date"},
                                      "device_id": {"type": "keyword"},
                                      "metric_name": {"type": "keyword"},
                                      "metric_value": {"type": "float"},
                                      "location": {"type": "keyword"},
                                      "zone": {"type": "keyword"}
                                  }
                              }
                          }
                          
                          req = urllib.request.Request(create_url, method='PUT')
                          req.add_header('Content-Type', 'application/json')
                          if password:
                              credentials = f"{username}:{password}"
                              encoded_credentials = base64.b64encode(credentials.encode()).decode()
                              req.add_header('Authorization', f'Basic {encoded_credentials}')
                          
                          urllib.request.urlopen(req, data=json.dumps(mapping).encode())
                          print(f"Index {index_name} created successfully")
                          return True
                      except Exception as create_e:
                          print(f"Error creating index: {create_e}")
                          return False
                  else:
                      print(f"Error checking index: {e}")
                      return False
              except Exception as e:
                  print(f"Error checking index: {e}")
                  return False
          
          def bulk_index_batch(batch_data):
              """Bulk index a batch of documents"""
              try:
                  url = f"http://{es_host}:{es_port}/_bulk"
                  
                  bulk_body = ""
                  for doc in batch_data:
                      bulk_body += json.dumps({"index": {"_index": index_name}}) + "\n"
                      bulk_body += json.dumps(doc) + "\n"
                  
                  req = urllib.request.Request(url, method='POST')
                  req.add_header('Content-Type', 'application/x-ndjson')
                  if password:
                      credentials = f"{username}:{password}"
                      encoded_credentials = base64.b64encode(credentials.encode()).decode()
                      req.add_header('Authorization', f'Basic {encoded_credentials}')
                  
                  response = urllib.request.urlopen(req, data=bulk_body.encode())
                  result = json.loads(response.read().decode())
                  
                  if result.get('errors', False):
                      print(f"Bulk indexing errors: {result}")
                      return False
                  else:
                      print(f"Successfully indexed batch of {len(batch_data)} documents")
                      return True
              except Exception as e:
                  print(f"Error bulk indexing batch: {e}")
                  return False
          
          if not create_index_if_not_exists():
              print("es_status=error")
              print("es_error=Failed to create index")
              exit(1)
          
          batch_size = 10000
          total_rows = 0
          successful_indexes = 0
          
          with open(csv_path, 'r', encoding='utf-8') as file:
              reader = csv.DictReader(file, delimiter='{{ inputs.delimiter }}')
              
              batch = []
              for row in reader:
                  total_rows += 1
                  
                  try:
                      timestamp = datetime.fromisoformat(row['timestamp'].replace('Z', '+00:00'))
                      timestamp_str = timestamp.isoformat()
                  except:
                      timestamp_str = row['timestamp']
                  
                  doc = {
                      "timestamp": timestamp_str,
                      "device_id": row['device_id'],
                      "metric_name": row['metric_name'],
                      "metric_value": float(row['metric_value']),
                      "location": row['location'],
                      "zone": row['zone']
                  }
                  
                  batch.append(doc)
                  
                  if len(batch) >= batch_size:
                      if bulk_index_batch(batch):
                          successful_indexes += len(batch)
                      batch = []
                      print(f"Processed {total_rows} rows so far...")
              
              if batch:
                  if bulk_index_batch(batch):
                      successful_indexes += len(batch)
          
          print(f"Total rows processed: {total_rows}")
          print(f"Successfully indexed: {successful_indexes}")
          
          print(f"es_rows_indexed={successful_indexes}")
          print("es_status=success" if successful_indexes > 0 else "es_status=error")
          if successful_indexes == 0:
              print("es_error=No documents were inserted successfully")

      - id: copy_data_to_qdrant
        type: io.kestra.plugin.scripts.python.Script
        description: "Insert CSV data into Qdrant vector database using Python"
        script: |
          import csv
          import json
          import urllib.request
          import urllib.parse
          import hashlib
          from datetime import datetime
          
          qdrant_host = "qdrant"  # Use service name in Docker network
          qdrant_port = "6333"
          collection_name = "{{ inputs.table_name }}"
          
          csv_path = "{{ outputs.download_file.uri }}"
          
          def create_collection_if_not_exists():
              """Create Qdrant collection if it doesn't exist"""
              try:
                  # Check if collection exists
                  url = f"http://{qdrant_host}:{qdrant_port}/collections/{collection_name}"
                  req = urllib.request.Request(url, method='GET')
                  
                  try:
                      response = urllib.request.urlopen(req)
                      print(f"Collection {collection_name} already exists")
                      return True
                  except urllib.error.HTTPError as e:
                      if e.code == 404:
                          # Create collection
                          create_url = f"http://{qdrant_host}:{qdrant_port}/collections"
                          collection_config = {
                              "vectors": {
                                  "size": 6,  # 6 dimensions for our metrics
                                  "distance": "Cosine"
                              },
                              "on_disk_payload": True
                          }
                          
                          req = urllib.request.Request(create_url, method='PUT')
                          req.add_header('Content-Type', 'application/json')
                          
                          urllib.request.urlopen(req, data=json.dumps(collection_config).encode())
                          print(f"Collection {collection_name} created successfully")
                          return True
                      else:
                          print(f"Error checking collection: {e}")
                          return False
              except Exception as e:
                  print(f"Error creating collection: {e}")
                  return False
          
          def insert_batch(batch_data):
              """Insert a batch of points into Qdrant"""
              try:
                  url = f"http://{qdrant_host}:{qdrant_port}/collections/{collection_name}/points"
                  
                  points = []
                  for i, row in enumerate(batch_data):
                      # Create a unique ID for each point
                      point_id = hashlib.md5(f"{row['timestamp']}_{row['device_id']}_{row['metric_name']}".encode()).hexdigest()
                      
                      # Convert metrics to vector (normalize values for better similarity search)
                      vector = [
                          float(row['metric_value']) / 100.0,  # Normalize metric value
                          hash(row['device_id']) % 100 / 100.0,  # Hash device_id to 0-1 range
                          hash(row['metric_name']) % 100 / 100.0,  # Hash metric_name to 0-1 range
                          hash(row['location']) % 100 / 100.0,  # Hash location to 0-1 range
                          hash(row['zone']) % 100 / 100.0,  # Hash zone to 0-1 range
                          float(row['timestamp'].split('T')[0].replace('-', '')) / 100000000.0  # Date as normalized value
                      ]
                      
                      point = {
                          "id": point_id,
                          "vector": vector,
                          "payload": {
                              "timestamp": row['timestamp'],
                              "device_id": row['device_id'],
                              "metric_name": row['metric_name'],
                              "metric_value": float(row['metric_value']),
                              "location": row['location'],
                              "zone": row['zone']
                          }
                      }
                      points.append(point)
                  
                  payload = {
                      "points": points
                  }
                  
                  req = urllib.request.Request(url, method='PUT')
                  req.add_header('Content-Type', 'application/json')
                  
                  response = urllib.request.urlopen(req, data=json.dumps(payload).encode())
                  result = json.loads(response.read().decode())
                  
                  if result.get('status') == 'ok':
                      print(f"Successfully inserted batch of {len(batch_data)} points")
                      return True
                  else:
                      print(f"Error inserting batch: {result}")
                      return False
              except Exception as e:
                  print(f"Error inserting batch: {e}")
                  return False
          try:
              url = f"http://{qdrant_host}:{qdrant_port}/collections/{collection_name}"
              req = urllib.request.Request(url, method='GET')
              response = urllib.request.urlopen(req)
              print(f"Collection {collection_name} verified successfully")
          except Exception as e:
              print(f"qdrant_status=error")
              print(f"qdrant_error=Collection {collection_name} not found: {e}")
              exit(1)
          
          batch_size = 1000  # Smaller batch size for Qdrant
          total_rows = 0
          successful_inserts = 0
          
          with open(csv_path, 'r', encoding='utf-8') as file:
              reader = csv.DictReader(file, delimiter='{{ inputs.delimiter }}')
              
              batch = []
              for row in reader:
                  total_rows += 1
                  
                  try:
                      timestamp = datetime.fromisoformat(row['timestamp'].replace('Z', '+00:00'))
                      timestamp_str = timestamp.isoformat()
                  except:
                      timestamp_str = row['timestamp']
                  
                  batch.append({
                      'timestamp': timestamp_str,
                      'device_id': row['device_id'],
                      'metric_name': row['metric_name'],
                      'metric_value': row['metric_value'],
                      'location': row['location'],
                      'zone': row['zone']
                  })
                  
                  if len(batch) >= batch_size:
                      if insert_batch(batch):
                          successful_inserts += len(batch)
                      batch = []
                      print(f"Processed {total_rows} rows so far...")
              
              if batch:
                  if insert_batch(batch):
                      successful_inserts += len(batch)
          
          print(f"Total rows processed: {total_rows}")
          print(f"Successfully inserted: {successful_inserts}")
          
          print(f"qdrant_points_inserted={successful_inserts}")
          print("qdrant_status=success" if successful_inserts > 0 else "qdrant_status=error")
          if successful_inserts == 0:
              print("qdrant_error=No points were inserted successfully")

  - id: cleanup
    type: io.kestra.plugin.core.storage.Delete
    uri: "{{ outputs.download_file.uri }}"

triggers:
  - id: schedule
    type: io.kestra.plugin.core.trigger.Schedule
    cron: "0 */6 * * *"
    description: "Every 6 hours"

errors:
  - id: error_handler
    type: io.kestra.plugin.core.log.Log
    message: "Error processing CSV: {{ error.message }}"
