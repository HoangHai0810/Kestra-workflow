id: s3-csv-ingestion
namespace: com.flowio
description: "Get CSV from S3 and insert into TimescaleDB, ClickHouse, Elasticsearch, and Qdrant"

inputs:
  - id: s3_bucket
    type: STRING
    defaults: "static-staging.flowio.app"
  - id: s3_key
    type: STRING
    defaults: "resources/kestra-test/sample-metrics.csv"
  - id: table_name
    type: STRING
    defaults: "test_metrics_data"
  - id: delimiter
    type: STRING
    defaults: ","

tasks:
  - id: download_file
    type: io.kestra.plugin.aws.s3.Download
    description: "Download CSV from S3"
    bucket: "{{ trigger.bucket | default(inputs.s3_bucket) }}"
    key: "{{ trigger.key | default(inputs.s3_key) }}"
    accessKeyId: "{{ secret('AWS_ACCESS_KEY_ID') }}"
    secretKeyId: "{{ secret('AWS_SECRET_ACCESS_KEY') }}"
    region: "{{ secret('AWS_REGION') }}"

  - id: create_tables
    type: io.kestra.plugin.core.flow.Parallel
    description: "Create tables in databases"
    tasks:
      - id: create_timescaledb_table
        type: io.kestra.plugin.jdbc.postgresql.Query
        description: "Create TimescaleDB table if not exists"
        url: "jdbc:postgresql://postgres:5432/kestra_data"
        username: "kestra_user"
        password: "kestra_password"
        sql: |
          CREATE TABLE IF NOT EXISTS {{ inputs.table_name }} (
            timestamp TIMESTAMPTZ,
            device_id TEXT,
            metric_name TEXT,
            metric_value DOUBLE PRECISION,
            location TEXT,
            zone TEXT
          );
          SELECT create_hypertable('{{ inputs.table_name }}', 'timestamp', if_not_exists => TRUE);

      - id: create_clickhouse_table
        type: io.kestra.plugin.scripts.python.Script
        description: "Create ClickHouse table using Python HTTP API"
        script: |
          import urllib.request
          import urllib.parse
          import base64

          clickhouse_host = "host.docker.internal"
          clickhouse_port = "8123"
          username = "{{ secret('CLICKHOUSE_USER') }}"
          password = "{{ secret('CLICKHOUSE_PASSWORD') }}"
          database = "kestra_data"
          table = "{{ inputs.table_name }}"

          url = f"http://{clickhouse_host}:{clickhouse_port}/"
          
          create_db_query = f"CREATE DATABASE IF NOT EXISTS {database}"
          params = urllib.parse.urlencode({'query': create_db_query})
          full_url = f"{url}?{params}"
          
          req = urllib.request.Request(full_url, method='POST')
          if password:
              credentials = f"{username}:{password}"
              encoded_credentials = base64.b64encode(credentials.encode()).decode()
              req.add_header('Authorization', f'Basic {encoded_credentials}')
          
          try:
              response = urllib.request.urlopen(req)
              print(f"Database {database} created/verified successfully")
          except Exception as e:
              print(f"Error creating database: {e}")
          
          create_table_query = f"""
          CREATE TABLE IF NOT EXISTS {database}.{table} (
              timestamp DateTime64(3),
              device_id String,
              metric_name String,
              metric_value Float64,
              location String,
              zone String
          ) ENGINE = MergeTree()
          ORDER BY (timestamp, device_id, metric_name)
          PARTITION BY toYYYYMM(timestamp)
          TTL timestamp + INTERVAL 1 YEAR
          """
          params = urllib.parse.urlencode({'query': create_table_query})
          full_url = f"{url}?{params}"
          
          req = urllib.request.Request(full_url, method='POST')
          if password:
              credentials = f"{username}:{password}"
              encoded_credentials = base64.b64encode(credentials.encode()).decode()
              req.add_header('Authorization', f'Basic {encoded_credentials}')
          
          try:
              response = urllib.request.urlopen(req)
              print(f"Table {table} created/verified successfully")
          except Exception as e:
              print(f"Error creating table: {e}")

      - id: create_elasticsearch_index
        type: io.kestra.plugin.scripts.python.Script
        description: "Create Elasticsearch index if not exists"
        script: |
          import urllib.request
          import json
          import base64

          es_host = "host.docker.internal"
          es_port = "9200"
          username = "{{ secret('ES_USER') }}"
          password = "{{ secret('ES_PASSWORD') }}"
          index_name = "{{ inputs.table_name }}"

          def create_index_if_not_exists():
              """Create Elasticsearch index if it doesn't exist"""
              try:
                  url = f"http://{es_host}:{es_port}/{index_name}"
                  req = urllib.request.Request(url, method='HEAD')
                  if password:
                      credentials = f"{username}:{password}"
                      encoded_credentials = base64.b64encode(credentials.encode()).decode()
                      req.add_header('Authorization', f'Basic {encoded_credentials}')
                      
                  try:
                      response = urllib.request.urlopen(req)
                      print(f"Index {index_name} already exists")
                      return True
                  except urllib.error.HTTPError as e:
                      if e.code == 404:
                          create_url = f"http://{es_host}:{es_port}/{index_name}"
                          mapping = {
                              "mappings": {
                                  "properties": {
                                      "timestamp": {"type": "date"},
                                      "device_id": {"type": "keyword"},
                                      "metric_name": {"type": "keyword"},
                                      "metric_value": {"type": "float"},
                                      "location": {"type": "keyword"},
                                      "zone": {"type": "keyword"}
                                  }
                              }
                          }
                          
                          req = urllib.request.Request(create_url, method='PUT')
                          req.add_header('Content-Type', 'application/json')
                          if password:
                              credentials = f"{username}:{password}"
                              encoded_credentials = base64.b64encode(credentials.encode()).decode()
                              req.add_header('Authorization', f'Basic {encoded_credentials}')
                          
                          urllib.request.urlopen(req, data=json.dumps(mapping).encode())
                          print(f"Index {index_name} created successfully")
                          return True
                      else:
                          print(f"Error checking index: {e}")
                          return False
              except Exception as e:
                  print(f"Error creating index: {e}")
                  return False

          if create_index_if_not_exists():
              print("elasticsearch_index_status=success")
          else:
              print("elasticsearch_index_status=error")
              print("elasticsearch_index_error=Failed to create index") 

      - id: create_qdrant_collection
        type: io.kestra.plugin.scripts.python.Script
        description: "Create Qdrant collection if not exists"
        script: |
          import urllib.request
          import json

          qdrant_host = "host.docker.internal"
          qdrant_port = "6333"
          collection_name = "{{ inputs.table_name }}"

          def create_collection_if_not_exists():
              """Create Qdrant collection if it doesn't exist"""
              try:
                  url = f"http://{qdrant_host}:{qdrant_port}/collections/{collection_name}"
                  req = urllib.request.Request(url, method='GET')
                  
                  try:
                      response = urllib.request.urlopen(req)
                      print(f"Collection {collection_name} already exists")
                      return True
                  except urllib.error.HTTPError as e:
                      if e.code == 404:
                          create_url = f"http://{qdrant_host}:{qdrant_port}/collections/{collection_name}"
                          collection_config = {
                              "vectors": {
                                  "size": 6,
                                  "distance": "Cosine"
                              },
                              "on_disk_payload": True
                          }
                          req = urllib.request.Request(create_url, method='PUT')
                          req.add_header('Content-Type', 'application/json')
                          response = urllib.request.urlopen(req, data=json.dumps(collection_config).encode())
                          print(f"Collection {collection_name} created successfully")
                          return True
                      else:
                          print(f"Error checking collection: {e}")
                          return False
              except Exception as e:
                  print(f"Error creating collection: {e}")
                  return False

          if create_collection_if_not_exists():
              print("qdrant_collection_status=success")
          else:
              print("qdrant_collection_status=error")
              print("qdrant_collection_error=Failed to create collection")

  - id: transform_data
    type: io.kestra.plugin.core.flow.Parallel
    description: "Transform data and insert into databases"
    tasks:
      - id: copy_data_to_timescaledb
        type: io.kestra.plugin.jdbc.postgresql.CopyIn
        description: "Insert CSV data into TimescaleDB"
        url: "jdbc:postgresql://postgres:5432/kestra_data"
        username: "kestra_user"
        password: "kestra_password"
        format: CSV
        from: "{{ outputs.download_file.uri }}"
        table: "{{ inputs.table_name }}"
        header: true
        delimiter: "{{ inputs.delimiter }}"
        columns:
          - timestamp
          - device_id
          - metric_name
          - metric_value
          - location
          - zone

      - id: copy_data_to_clickhouse
        type: io.kestra.plugin.scripts.python.Script
        description: "Insert CSV data into ClickHouse using Python HTTP API"
        script: |
          import csv
          import urllib.request
          import urllib.parse
          import base64
          import json
          from pathlib import Path
          from datetime import datetime
          
          clickhouse_host = "host.docker.internal"
          clickhouse_port = "8123"
          username = "{{ secret('CLICKHOUSE_USER') }}"
          password = "{{ secret('CLICKHOUSE_PASSWORD') }}"
          database = "kestra_data"
          table = "{{ inputs.table_name }}"
          batch_size = 10000
          
          def insert_batch(batch_data, db, tbl, host, port, user, pwd):
              """Insert a batch of data into ClickHouse"""
              try:
                  url = f"http://{host}:{port}/"
                  
                  insert_query = f"INSERT INTO {db}.{tbl} (timestamp, device_id, metric_name, metric_value, location, zone) VALUES"
                  
                  values_list = []
                  for row in batch_data:
                      timestamp = str(row[0]).replace("'", "''")
                      device_id = str(row[1]).replace("'", "''")
                      metric_name = str(row[2]).replace("'", "''")
                      metric_value = float(row[3])
                      location = str(row[4]).replace("'", "''")
                      zone = str(row[5]).replace("'", "''")
                      
                      values_list.append(f"('{timestamp}', '{device_id}', '{metric_name}', {metric_value}, '{location}', '{zone}')")
                  
                  values_str = ",".join(values_list)
                  full_query = f"{insert_query} {values_str}"
                
                  
                  params = urllib.parse.urlencode({'query': full_query})
                  full_url = f"{url}?{params}"
                  
                  req = urllib.request.Request(full_url, method='POST')
                  if pwd:
                      credentials = f"{user}:{pwd}"
                      encoded_credentials = base64.b64encode(credentials.encode()).decode()
                      req.add_header('Authorization', f'Basic {encoded_credentials}')
                  
                  response = urllib.request.urlopen(req)
                  response_text = response.read().decode()
                  
                  if response.getcode() == 200:
                      print(f"Successfully inserted batch of {len(batch_data)} rows")
                      if response_text:
                          print(f"Response: {response_text}")
                      return True
                  else:
                      print(f"Failed to insert batch: HTTP {response.getcode()}")
                      print(f"Response: {response_text}")
                      return False
                      
              except Exception as e:
                  print(f"Error inserting batch: {e}")
                  if hasattr(e, 'read'):
                      try:
                          error_response = e.read().decode()
                          print(f"Error response: {error_response}")
                      except:
                          pass
                  return False
          
          csv_path = "{{ outputs.download_file.uri }}"
          
          total_rows = 0
          successful_inserts = 0
          
          with open(csv_path, 'r', encoding='utf-8') as file:
              next(file)
              reader = csv.reader(file, delimiter='{{ inputs.delimiter }}')
              
              batch = []
              for row in reader:
                  total_rows += 1
                  
                  try:
                      timestamp = datetime.fromisoformat(row[0].replace('Z', '+00:00'))
                      timestamp_str = timestamp.strftime('%Y-%m-%d %H:%M:%S')
                  except:
                      timestamp_str = row[0]
                  
                  batch.append([
                      timestamp_str,
                      row[1],  # device_id
                      row[2],  # metric_name
                      row[3],  # metric_value
                      row[4],  # location
                      row[5]   # zone
                  ])
                  
                  if len(batch) >= batch_size:
                      if insert_batch(batch, database, table, clickhouse_host, clickhouse_port, username, password):
                          successful_inserts += len(batch)
                      batch = []
                      print(f"Processed {total_rows} rows so far...")
              
              if batch:
                  if insert_batch(batch, database, table, clickhouse_host, clickhouse_port, username, password):
                      successful_inserts += len(batch)
          
          print(f"Total rows processed: {total_rows}")
          print(f"Successfully inserted: {successful_inserts}")
          
          print(f"clickhouse_rows_inserted={successful_inserts}")
          print("clickhouse_status=success" if successful_inserts > 0 else "clickhouse_status=error")
          if successful_inserts == 0:
              print("clickhouse_error=No rows were inserted successfully")

      - id: copy_data_to_elasticsearch
        type: io.kestra.plugin.scripts.python.Script
        description: "Bulk load CSV data into Elasticsearch using Python"
        script: |
          import csv
          import json
          import urllib.request
          import base64
          from datetime import datetime

          es_host = "host.docker.internal"
          es_port = "9200"
          username = "{{ secret('ES_USER') }}"
          password = "{{ secret('ES_PASSWORD') }}"
          index_name = "{{ inputs.table_name }}"
          csv_path = "{{ outputs.download_file.uri }}"

          def create_document(doc_id, row_data):
              """Create Elasticsearch document from CSV row"""
              try:
                  timestamp_str = row_data['timestamp']
                  try:
                      if 'T' in timestamp_str and 'Z' in timestamp_str:
                          timestamp = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))
                          timestamp_str = timestamp.isoformat()
                  except Exception as e:
                          pass

                  doc = {
                      "timestamp": timestamp_str,
                      "device_id": row_data['device_id'],
                      "metric_name": row_data['metric_name'],
                      "metric_value": float(row_data['metric_value']),
                      "location": row_data['location'],
                      "zone": row_data['zone']
                  }
                  return doc
              except Exception as e:
                  print(f"Error creating document from row: {e}")
                  return None

          def bulk_insert(documents, batch_size=1000):
              """Insert documents in bulk"""
              try:
                  if not documents:
                      return 0

                  bulk_data = ""
                  for doc in documents:
                      index_action = {
                          "index": {
                              "_index": index_name,
                              "_id": f"{doc['device_id']}_{doc['metric_name']}_{doc['timestamp']}"
                          }
                      }
                      bulk_data += json.dumps(index_action) + "\n"
                      
                      bulk_data += json.dumps(doc) + "\n"

                  url = f"http://{es_host}:{es_port}/_bulk"
                  req = urllib.request.Request(url, method='POST', data=bulk_data.encode('utf-8'))
                  req.add_header('Content-Type', 'application/x-ndjson')
                  
                  if password:
                      credentials = f"{username}:{password}"
                      encoded_credentials = base64.b64encode(credentials.encode()).decode()
                      req.add_header('Authorization', f'Basic {encoded_credentials}')

                  response = urllib.request.urlopen(req)
                  result = json.loads(response.read().decode())
                  
                  if result.get('errors', True):
                      print(f"Bulk insert errors: {result}")
                      return 0
                  else:
                      print(f"Successfully inserted {len(documents)} documents")
                      return len(documents)
                      
              except Exception as e:
                  print(f"Error in bulk insert: {e}")
                  return 0

          try:
              url = f"http://{es_host}:{es_port}/{index_name}"
              req = urllib.request.Request(url, method='HEAD')
              if password:
                  credentials = f"{username}:{password}"
                  encoded_credentials = base64.b64encode(credentials.encode()).decode()
                  req.add_header('Authorization', f'Basic {encoded_credentials}')

              response = urllib.request.urlopen(req)
              print(f"Index {index_name} verified successfully")
          except Exception as e:
              print(f"elasticsearch_status=error")
              print(f"elasticsearch_error=Index {index_name} not found: {e}")
              exit(1)
                      
          try:
              url = f"http://{es_host}:{es_port}/{index_name}"
              req = urllib.request.Request(url, method='GET')
              response = urllib.request.urlopen(req)
              print(f"Index {index_name} verified successfully")
              print(f"elasticsearch_index_status=success")
          except Exception as e:
              print(f"elasticsearch_index_status=error")
              print(f"elasticsearch_index_error=Index {index_name} not found: {e}")
              exit(1)

          total_rows = 0
          successful_inserts = 0
          batch_size = 1000

          with open(csv_path, 'r', encoding='utf-8') as file:
              next(file)
              reader = csv.reader(file, delimiter='{{ inputs.delimiter }}')
              
              batch = []
              for row in reader:
                  total_rows += 1
                  
                  row_dict = {
                      'timestamp': row[0],
                      'device_id': row[1],
                      'metric_name': row[2],
                      'metric_value': row[3],
                      'location': row[4],
                      'zone': row[5]
                  }
                  
                  doc = create_document(total_rows, row_dict)
                  if doc:
                      batch.append(doc)
                  
                  if len(batch) >= batch_size:
                      if bulk_insert(batch, batch_size):
                          successful_inserts += len(batch)
                      batch = []
                      print(f"Processed {total_rows} rows so far...")
              
              if batch:
                  if bulk_insert(batch, batch_size):
                      successful_inserts += len(batch)

          print(f"Total rows processed: {total_rows}")
          print(f"Successfully inserted: {successful_inserts}")
          
          print(f"elasticsearch_documents_inserted={successful_inserts}")
          print("elasticsearch_status=success" if successful_inserts > 0 else "elasticsearch_status=error")
          if successful_inserts == 0:
              print("elasticsearch_error=No documents were inserted successfully")

      - id: copy_data_to_qdrant
        type: io.kestra.plugin.scripts.python.Script
        description: "Insert CSV data into Qdrant vector database using Python"
        script: |
          import csv
          import json
          import urllib.request
          import hashlib
          from datetime import datetime

          qdrant_host = "host.docker.internal"
          qdrant_port = "6333"
          collection_name = "{{ inputs.table_name }}"
          csv_path = "{{ outputs.download_file.uri }}"

          def insert_batch(batch_data):
              """Insert a batch of points into Qdrant"""
              try:
                  url = f"http://{qdrant_host}:{qdrant_port}/collections/{collection_name}/points"
                  
                  points = []
                  for row in batch_data:
                      point_id = hashlib.md5(f"{row['timestamp']}_{row['device_id']}_{row['metric_name']}".encode()).hexdigest()
                      
                      vector = [
                          float(row['metric_value']) / 100.0,
                          hash(row['device_id']) % 100 / 100.0,
                          hash(row['metric_name']) % 100 / 100.0,
                          hash(row['location']) % 100 / 100.0,
                          hash(row['zone']) % 100 / 100.0,
                          float(row['timestamp'].split('T')[0].replace('-', '')) / 100000000.0
                      ]
                      
                      point = {
                          "id": point_id,
                          "vector": vector,
                          "payload": {
                              "timestamp": row['timestamp'],
                              "device_id": row['device_id'],
                              "metric_name": row['metric_name'],
                              "metric_value": float(row['metric_value']),
                              "location": row['location'],
                              "zone": row['zone']
                          }
                      }
                      points.append(point)
                  
                  payload = {
                      "points": points
                  }
                  
                  req = urllib.request.Request(url, method='PUT')
                  req.add_header('Content-Type', 'application/json')
                  
                  response = urllib.request.urlopen(req, data=json.dumps(payload).encode())
                  result = json.loads(response.read().decode())
                  
                  if result.get('status') == 'ok':
                      print(f"Successfully inserted batch of {len(batch_data)} points")
                      return True
                  else:
                      print(f"Error inserting batch: {result}")
                      return False
              except Exception as e:
                  print(f"Error inserting batch: {e}")
                  return False

          try:
              url = f"http://{qdrant_host}:{qdrant_port}/collections/{collection_name}"
              req = urllib.request.Request(url, method='GET')
              response = urllib.request.urlopen(req)
              print(f"Collection {collection_name} verified successfully")
          except Exception as e:
              print(f"qdrant_status=error")
              print(f"qdrant_error=Collection {collection_name} not found: {e}")
              exit(1)

          batch_size = 1000
          total_rows = 0
          successful_inserts = 0

          with open(csv_path, 'r', encoding='utf-8') as file: 
              next(file)
              reader = csv.reader(file, delimiter='{{ inputs.delimiter }}')
              
              batch = []
              for row in reader:
                  total_rows += 1
                  
                  try:
                      timestamp = datetime.fromisoformat(row[0].replace('Z', '+00:00'))
                      timestamp_str = timestamp.isoformat()
                  except:
                      timestamp_str = row[0]
                  
                  batch.append({
                      'timestamp': timestamp_str,
                      'device_id': row[1],
                      'metric_name': row[2],
                      'metric_value': row[3],
                      'location': row[4],
                      'zone': row[5]
                  })
                  
                  if len(batch) >= batch_size:
                      if insert_batch(batch):
                          successful_inserts += len(batch)
                      batch = []
                      print(f"Processed {total_rows} rows so far...")
              
              if batch:
                  if insert_batch(batch):
                      successful_inserts += len(batch)

          print(f"Total rows processed: {total_rows}")
          print(f"Successfully inserted: {successful_inserts}")
          
          print(f"qdrant_points_inserted={successful_inserts}")
          print("qdrant_status=success" if successful_inserts > 0 else "qdrant_status=error")
          if successful_inserts == 0:
              print("qdrant_error=No points were inserted successfully")

  - id: cleanup
    type: io.kestra.plugin.core.storage.Delete
    uri: "{{ outputs.download_file.uri }}"

  - id: delete_source_s3_object
    type: io.kestra.plugin.aws.s3.Delete
    description: "Delete source S3 object after successful processing"
    bucket: "{{ trigger.bucket | default(inputs.s3_bucket) }}"
    key: "{{ trigger.key | default(inputs.s3_key) }}"
    accessKeyId: "{{ secret('AWS_ACCESS_KEY_ID') }}"
    secretKeyId: "{{ secret('AWS_SECRET_ACCESS_KEY') }}"
    region: "{{ secret('AWS_REGION') }}"

triggers:
  - id: delete_processed_files
    type: io.kestra.plugin.aws.s3.Trigger
    description: "Run when new CSV object is added to S3"
    bucket: "{{ trigger.bucket | default(inputs.s3_bucket) }}"
    accessKeyId: "{{ secret('AWS_ACCESS_KEY_ID') }}"
    secretKeyId: "{{ secret('AWS_SECRET_ACCESS_KEY') }}"
    region: "{{ secret('AWS_REGION') }}"
    prefix: "resources/kestra-test/"
    regexp: ".*\\.csv$"
    interval: PT1M
    action: NONE

errors:
  - id: error_handler
    type: io.kestra.plugin.core.log.Log
    message: "Flow failed. Check task logs for details."
